"""This type stub file was generated by pyright.
"""

import dataclasses
import sys
from collections import deque
from collections.abc import Callable, Hashable, Sequence
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    Generic,
    Literal,
    NamedTuple,
    Self,
    TypeVar,
)

from langchain_core.messages.tool import ToolOutputMixin
from langchain_core.runnables import Runnable, RunnableConfig
from langgraph.checkpoint.base import BaseCheckpointSaver, CheckpointMetadata
from langgraph.store.base import BaseStore
from typing_extensions import TypedDict

if TYPE_CHECKING: ...
type All = Literal["*"]
type Checkpointer = None | Literal[False] | BaseCheckpointSaver
type StreamMode = Literal["values", "updates", "debug", "messages", "custom"]
type StreamWriter = Callable[[Any], None]
if sys.version_info >= (3, 10):
    _DC_KWARGS = ...
else: ...

def default_retry_on(exc: Exception) -> bool: ...

class RetryPolicy(NamedTuple):
    """Configuration for retrying nodes."""

    initial_interval: float = ...
    backoff_factor: float = ...
    max_interval: float = ...
    max_attempts: int = ...
    jitter: bool = ...
    retry_on: type[Exception] | Sequence[type[Exception]] | Callable[[Exception], bool] = ...

class CachePolicy(NamedTuple):
    """Configuration for caching nodes."""


@dataclasses.dataclass(**_DC_KWARGS)
class Interrupt:
    value: Any
    resumable: bool = ...
    ns: Sequence[str] | None = ...
    when: Literal["during"] = ...

class PregelTask(NamedTuple):
    id: str
    name: str
    path: tuple[str | int | tuple, ...]
    error: Exception | None = ...
    interrupts: tuple[Interrupt, ...] = ...
    state: None | RunnableConfig | StateSnapshot = ...
    result: dict[str, Any] | None = ...

class PregelExecutableTask(NamedTuple):
    name: str
    input: Any
    proc: Runnable
    writes: deque[tuple[str, Any]]
    config: RunnableConfig
    triggers: list[str]
    retry_policy: RetryPolicy | None
    cache_policy: CachePolicy | None
    id: str
    path: tuple[str | int | tuple, ...]
    scheduled: bool = ...
    writers: Sequence[Runnable] = ...

class StateSnapshot(NamedTuple):
    """Snapshot of the state of the graph at the beginning of a step."""

    values: dict[str, Any] | Any
    next: tuple[str, ...]
    config: RunnableConfig
    metadata: CheckpointMetadata | None
    created_at: str | None
    parent_config: RunnableConfig | None
    tasks: tuple[PregelTask, ...]

class Send:
    """A message or packet to send to a specific node in the graph.

    The `Send` class is used within a `StateGraph`'s conditional edges to
    dynamically invoke a node with a custom state at the next step.

    Importantly, the sent state can differ from the core graph's state,
    allowing for flexible and dynamic workflow management.

    One such example is a "map-reduce" workflow where your graph invokes
    the same node multiple times in parallel with different states,
    before aggregating the results back into the main graph's state.

    Attributes:
        node (str): The name of the target node to send the message to.
        arg (Any): The state or message to send to the target node.

    Examples:
        >>> from typing import Annotated
        >>> import operator
        >>> class OverallState(TypedDict):
        ...     subjects: list[str]
        ...     jokes: Annotated[list[str], operator.add]
        >>> from langgraph.types import Send
        >>> from langgraph.graph import END, START
        >>> def continue_to_jokes(state: OverallState):
        ...     return [Send("generate_joke", {"subject": s}) for s in state["subjects"]]
        >>> from langgraph.graph import StateGraph
        >>> builder = StateGraph(OverallState)
        >>> builder.add_node(
        ...     "generate_joke", lambda state: {"jokes": [f"Joke about {state['subject']}"]}
        ... )
        >>> builder.add_conditional_edges(START, continue_to_jokes)
        >>> builder.add_edge("generate_joke", END)
        >>> graph = builder.compile()
        >>>
        >>> # Invoking with two subjects results in a generated joke for each
        >>> graph.invoke({"subjects": ["cats", "dogs"]})
        {'subjects': ['cats', 'dogs'], 'jokes': ['Joke about cats', 'Joke about dogs']}
    """

    __slots__ = ...
    node: str
    arg: Any
    def __init__(self, /, node: str, arg: Any) -> None:
        """Initialize a new instance of the Send class.

        Args:
            node (str): The name of the target node to send the message to.
            arg (Any): The state or message to send to the target node.
        """

    def __hash__(self) -> int: ...
    def __eq__(self, value: object) -> bool: ...

N = TypeVar("N", bound=Hashable)

@dataclasses.dataclass(**_DC_KWARGS)
class Command(Generic[N], ToolOutputMixin):
    """One or more commands to update the graph's state and send messages to nodes.

    Args:
        graph: graph to send the command to. Supported values are:

            - None: the current graph (default)
            - Command.PARENT: closest parent graph
        update: update to apply to the graph's state.
        resume: value to resume execution with. To be used together with [`interrupt()`][langgraph.types.interrupt].
        goto: can be one of the following:

            - name of the node to navigate to next (any node that belongs to the specified `graph`)
            - sequence of node names to navigate to next
            - `Send` object (to execute a node with the input provided)
            - sequence of `Send` objects
    """

    graph: str | None = ...
    update: Any | None = ...
    resume: Any | dict[str, Any] | None = ...
    goto: Send | Sequence[Send | str] | str = ...

    PARENT: ClassVar[Literal["__parent__"]] = ...

type StreamChunk = tuple[tuple[str, ...], str, Any]

class StreamProtocol:
    __slots__ = ...
    modes: set[StreamMode]
    __call__: Callable[[Self, StreamChunk], None]
    def __init__(
        self, __call__: Callable[[StreamChunk], None], modes: set[StreamMode]
    ) -> None: ...

class LoopProtocol:
    config: RunnableConfig
    store: BaseStore | None
    stream: StreamProtocol | None
    step: int
    stop: int
    def __init__(
        self,
        *,
        step: int,
        stop: int,
        config: RunnableConfig,
        store: BaseStore | None = ...,
        stream: StreamProtocol | None = ...,
    ) -> None: ...

class PregelScratchpad(TypedDict, total=False):
    interrupt_counter: int
    used_null_resume: bool
    resume: list[Any]

def interrupt(value: Any) -> Any:
    """Interrupt the graph with a resumable exception from within a node.

    The `interrupt` function enables human-in-the-loop workflows by pausing graph
    execution and surfacing a value to the client. This value can communicate context
    or request input required to resume execution.

    In a given node, the first invocation of this function raises a `GraphInterrupt`
    exception, halting execution. The provided `value` is included with the exception
    and sent to the client executing the graph.

    A client resuming the graph must use the [`Command`][langgraph.types.Command]
    primitive to specify a value for the interrupt and continue execution.
    The graph resumes from the start of the node, **re-executing** all logic.

    If a node contains multiple `interrupt` calls, LangGraph matches resume values
    to interrupts based on their order in the node. This list of resume values
    is scoped to the specific task executing the node and is not shared across tasks.

    To use an `interrupt`, you must enable a checkpointer, as the feature relies
    on persisting the graph state.

    Example:
        ```python
        import uuid
        from typing import Optional
        from typing_extensions import TypedDict

        from langgraph.checkpoint.memory import MemorySaver
        from langgraph.constants import START
        from langgraph.graph import StateGraph
        from langgraph.types import interrupt


        class State(TypedDict):
            \"\"\"The graph state.\"\"\"

            foo: str
            human_value: Optional[str]
            \"\"\"Human value will be updated using an interrupt.\"\"\"


        def node(state: State):
            answer = interrupt(
                # This value will be sent to the client
                # as part of the interrupt information.
                \"what is your age?\"
            )
            print(f\"> Received an input from the interrupt: {answer}\")
            return {\"human_value\": answer}


        builder = StateGraph(State)
        builder.add_node(\"node\", node)
        builder.add_edge(START, \"node\")

        # A checkpointer must be enabled for interrupts to work!
        checkpointer = MemorySaver()
        graph = builder.compile(checkpointer=checkpointer)

        config = {
            \"configurable\": {
                \"thread_id\": uuid.uuid4(),
            }
        }

        for chunk in graph.stream({\"foo\": \"abc\"}, config):
            print(chunk)
        ```

        ```pycon
        {
            "__interrupt__": (
                Interrupt(
                    value="what is your age?",
                    resumable=True,
                    ns=["node:62e598fa-8653-9d6d-2046-a70203020e37"],
                    when="during",
                ),
            )
        }
        ```

        ```python
        command = Command(resume=\"some input from a human!!!\")

        for chunk in graph.stream(Command(resume=\"some input from a human!!!\"), config):
            print(chunk)
        ```

        ```pycon
        Received an input from the interrupt: some input from a human!!!
        {'node': {'human_value': 'some input from a human!!!'}}
        ```

    Args:
        value: The value to surface to the client when the graph is interrupted.

    Returns:
        Any: On subsequent invocations within the same node (same task to be precise), returns the value provided during the first invocation

    Raises:
        GraphInterrupt: On the first invocation within the node, halts execution and surfaces the provided value to the client.
    """
